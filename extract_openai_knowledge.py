#!/usr/bin/env python3
"""
Extract Training Data from Existing OpenAI Detections

Every item we've detected with OpenAI is valuable training data!
This script:
1. Extracts all items from completed jobs (already paid for!)
2. Creates training dataset from OpenAI's detections
3. Identifies high-confidence vs low-confidence detections
4. Builds largest possible training set

This is "knowledge distillation" - learning from a teacher model (OpenAI)
to train our student model (PrimeHaul AI).
"""

import os
import json
import uuid
from datetime import datetime
from pathlib import Path
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from app.models import Item, Room, Job, Photo, TrainingDataset
from dotenv import load_dotenv

load_dotenv()


def extract_openai_detections():
    """Extract all existing OpenAI detections as training data"""

    print("=" * 80)
    print("ðŸ§  EXTRACTING OPENAI KNOWLEDGE - Knowledge Distillation")
    print("=" * 80)

    database_url = os.getenv("DATABASE_URL", "postgresql://primehaul@localhost/primehaul_local")
    engine = create_engine(database_url)
    Session = sessionmaker(bind=engine)
    session = Session()

    # Get all items that have been AI-detected
    all_items = session.query(Item).all()

    print(f"\nðŸ“Š Found {len(all_items)} AI-detected items in database")

    # Statistics
    stats = {
        'total_items': 0,
        'with_confidence': 0,
        'high_confidence': 0,  # > 0.8
        'medium_confidence': 0,  # 0.5 - 0.8
        'low_confidence': 0,  # < 0.5
        'categories': {},
        'items_with_photos': 0
    }

    training_samples = []

    for item in all_items:
        stats['total_items'] += 1

        # Get room and photos
        room = session.query(Room).filter(Room.id == item.room_id).first()
        if not room:
            continue

        photos = session.query(Photo).filter(Photo.room_id == room.id).all()
        photo_urls = [photo.storage_path for photo in photos] if photos else []

        if photo_urls:
            stats['items_with_photos'] += 1

        # Track confidence (OpenAI detections are fairly reliable - use 0.85)
        confidence = 0.85  # Assume OpenAI detections are high quality

        stats['with_confidence'] += 1
        stats['high_confidence'] += 1  # Count all as high confidence

        # Track categories
        category = item.item_category or 'furniture'  # Default to furniture if not set
        stats['categories'][category] = stats['categories'].get(category, 0) + 1

        # Calculate CBM
        cbm = float(item.cbm) if item.cbm else 0

        training_sample = {
            'source': 'openai_detection',
            'item_id': str(item.id),
            'item_name': item.name,
            'item_category': category,
            'length_cm': float(item.length_cm) if item.length_cm else None,
            'width_cm': float(item.width_cm) if item.width_cm else None,
            'height_cm': float(item.height_cm) if item.height_cm else None,
            'cbm': cbm,
            'weight_kg': float(item.weight_kg) if item.weight_kg else None,
            'is_bulky': item.bulky,
            'is_fragile': item.fragile,
            'confidence': confidence,
            'photo_urls': photo_urls,
            'notes': item.notes,
            # OpenAI gave us this data - it's our teacher
            'teacher_model': 'gpt-4-vision',
            'verified': confidence > 0.8  # High confidence = trust it
        }

        training_samples.append(training_sample)

    print("\nðŸ“ˆ Extraction Statistics:")
    print(f"   Total items extracted: {stats['total_items']}")
    print(f"   Items with confidence scores: {stats['with_confidence']}")
    print(f"   High confidence (>0.8): {stats['high_confidence']}")
    print(f"   Medium confidence (0.5-0.8): {stats['medium_confidence']}")
    print(f"   Low confidence (<0.5): {stats['low_confidence']}")
    print(f"   Items with photos: {stats['items_with_photos']}")

    print("\nðŸ“¦ Categories Found:")
    for category, count in sorted(stats['categories'].items(), key=lambda x: x[1], reverse=True)[:15]:
        print(f"   {category:25s} â†’ {count:4d} samples")

    # Save to training_dataset table
    added_to_db = 0
    for sample in training_samples:
        # Check if already exists
        existing = session.query(TrainingDataset).filter(
            TrainingDataset.item_name == sample['item_name'],
            TrainingDataset.source_type == 'openai_detection'
        ).first()

        if not existing:
            # Add to training_dataset table
            training_entry = TrainingDataset(
                id=uuid.uuid4(),
                image_url=sample['photo_urls'][0] if sample['photo_urls'] else None,
                image_hash=None,  # TODO: calculate hash
                item_name=sample['item_name'],
                item_category=sample['item_category'],
                length_cm=sample['length_cm'],
                width_cm=sample['width_cm'],
                height_cm=sample['height_cm'],
                cbm=sample['cbm'],
                weight_kg=sample['weight_kg'],
                is_bulky=sample['is_bulky'],
                is_fragile=sample['is_fragile'],
                packing_requirement='none',
                source_type='openai_detection',
                source_id=uuid.UUID(sample['item_id']),
                confidence_score=sample['confidence'],
                verified=sample['verified'],
                used_in_training=False
            )
            session.add(training_entry)
            added_to_db += 1

    session.commit()

    print(f"\nðŸ’¾ Added {added_to_db} new samples to training_dataset table")

    # Export to JSON
    output_file = Path("training_data_cache") / f"openai_extracted_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    output_file.parent.mkdir(exist_ok=True)

    export_data = {
        'extracted_at': datetime.now().isoformat(),
        'total_samples': len(training_samples),
        'statistics': stats,
        'samples': training_samples
    }

    with open(output_file, 'w') as f:
        json.dump(export_data, f, indent=2)

    print(f"ðŸ“„ Exported to {output_file}")

    session.close()

    return export_data


if __name__ == "__main__":
    data = extract_openai_detections()

    print("\n" + "=" * 80)
    print("âœ… KNOWLEDGE EXTRACTION COMPLETE!")
    print("=" * 80)
    print(f"\nWe now have {data['total_samples']} training samples from OpenAI")
    print("These detections were already paid for - now we're learning from them!")
    print("\nNext: Run generate_synthetic_data.py to create even more training data")
